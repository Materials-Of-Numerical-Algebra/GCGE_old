2018.10.12 liyu

hypre lobpcg中 HYPRE_lobpcg.c中对于tol有初始设置相对误差和绝对误差初始是1.0e-06

下面是lobpcg的收敛准则
175   notConverged = 0;
176   for ( i = 0; i < n; i++ ) {
177     if ( utilities_FortranMatrixValue( resNorms,  i + 1,  1 ) >
178          utilities_FortranMatrixValue( lambda,  i + 1,  1 )*rtol + atol
179          + DBL_EPSILON ) {
180       activeMask[i] = 1;
181       notConverged++; 
182     }
183     else 
184       activeMask[i] = 0;
185   }


2018.10.14 liyu

在csr的test中, test_orth_ops.c是对于正交化成为ops的一个成员函数进行的尝试, 可以考虑以后在kona中利用这种做法. 
另外, test_orth_blas.c是对double型向量组进行正交化的函数, 已通过blas进行加速, 形式参数与test_orth_ops中的正交化函数类似. 
之后在GCGE中应该内置blas库, 进行编译, 而且都利用宏BLASname(...), LAPACKname(...)进行调用, 以防调用机器已有库时出现名字不统一
尤其是调用利用基于特定机器的blas lapack库
eigsol.c是对GCG核心函数的一个整理, 整个代码流程和函数形式参数设定是最合理的. 而且也是可以写到ops中的形式. 

2018.10.15 zhangning

修改需要的内存空间
在GCGE_WORKSPACE结构体中添加了

    GCGE_INT   V_size;
    GCGE_INT   V_tmp_size;
    GCGE_INT   CG_p_size;
    //GCGE_INT   CG_r_size;
    //GCGE_INT   RitzVec_size;

在gcge_workspace.c中添加

    GCGE_INT   V_size = max_dim_x + 2 * block_size;
    GCGE_INT   V_tmp_size = max_dim_x - nev + block_size;
    GCGE_INT   CG_p_size = block_size;

工作空间不再需要RitzVec, 因此涉及到RitzVec的子函数都要修改

用到RitzVec的子函数: 
1. gcge_xpw.h与gcge_xpw.c中 ComputeX, 修改形式参数为
    void GCGE_ComputeX(void **V, GCGE_DOUBLE *subspace_evec, void **X, 
        GCGE_INT size_X, GCGE_INT size_V,
        GCGE_OPS *ops, GCGE_WORKSPACE *workspace);
   gcge_eigsol.c中调用方式修改为
    GCGE_ComputeX(V, subspace_evec, evec, nev, workspace->dim_xpw, ops, workspace);
2. gcge_eigsol.c中MultiVecSwap, RitzVec替换为evec, 交换的位置由dim_x改为nev,
    且计算结束后不再需要将RitzVec交换给evec
3. gcge_eigsol.c中调用CheckConvergence时给的参数由RitzVec替换为evec
4. gcge_cg.c中CG迭代的临时空间用到了workspace->RitzVec, 改为使用workspace->evec
5. gcge_xpw.c中ComputeP时要把X重特征值的部分一起算,在线性组合与指针交换的地方修改向量个数
6. gcge_eigsol.c中span[X,W]中计算子空间矩阵前, 先令dim_xp=0(第298行)
7. gcge_rayleighritz.c中计算子空间矩阵时,调用下面这个函数时,最后一个参数由workspace->V_tmp改为
    workspace->evec,因为第一次迭代中需要nev大小的向量空间
    void GCGE_ComputeSubspaceMatrixVTAW(void **V, void *A, GCGE_INT start_W, 
            GCGE_INT end_V, GCGE_DOUBLE *subspace_mat, GCGE_OPS *ops, void **workspace)

2018.10.16 zhangning
1. 在gcge_orthogonal.c中添加了GCGE_BOrthogonal, 并将gcge_eigsol.c中对原GCGE_Orthogonal的调用形参做了修改(by hhxie)
2. 在gcge_cg.c中添加了块cg迭代(by hhxie)
    1) gcge_cg.c gcge_cg.h中添加了GCGE_BCG
    2) gcge_xpw.c 中 GCGE_ComputeW 做了修改, 改成多个W向量统一计算的格式

2018.10.17 zhangning
添加了SLEPc的调用接口, 包括 external/slepc, include/app/gcge_app_slepc.h, src/app/slepc, test/slepc

2018.10.18 zhangning
修改了gcge_rayleighritz.c中的GCGE_ComputeSubspaceMatrix,也修改了gcge_workspace.c中的subspace_dtmp的空间分配

2018.10.19 liyu
在求解特征值的程序中设置workspace->evec = solver->evec是不合适的, 因为在BCG中还要调用workspace->evec
也就是说, BCG不能单独被调用, 而且BCG的形式参数也很不好, 并不是一个一般性的接口. 
另外, 即使用了hypre_seqvecinnerprod做为局部内积, 好像BCG无论是原来的全局内积还是局部内积, 都会收敛, 很奇怪！
后来发现这样确实是对的, 从理论上讲. 

test_bcg.c在HYPRE下测试了BCG, 同时利用AMG做预条件子. 
之所以lobpcg不精确求解残量方程, 首先是它的正交化过程不能处理线性相关, 且精度不能太高. 
而GCGE精确求解线性方程可能会使得整个迭代次数减少, 进而减少W正交化的时间. 

所以, 外部求解器起始就是一个预条件子, 无论是否提供外部求解器, 都会进行BCG, 这是性价比最高的调用方式

2018.10.21 zhangning
1. 添加GCGE_CBOrthogonal时发现, ops->MultiVecLinearComb这个函数, 线性组合的系数应该是从系数矩阵的第0个元素开始取才合理
   之前用到的这个函数, 两个向量组都是从第0个开始用, 所以对之前的使用没有影响
   1) 修改了gcge_ops.h中对于ops->MultiVecLinearComb的注释
   2) 修改了gcge_ops.c中的GCGE_Default_MultiVecLinearComb
   3) 修改了gcge_app_slepc.c中的GCGE_SLEPC_MultiVecLinearComb
2. 修改了gcge_app_slepc.c中的GCGE_SLEPC_MultiVecAxpby(之前只将其作为copy函数, GCGE_CBOrthogonal调用时会出问题)
   1) 修改了gcge_app_slepc.c
3. 在gcge_orthogonal.c中添加了GCGE_CBOrthogonal函数, 表示Classical Block Orthogonal,消息发送次数少, 但会损失正交性
   可以在精度要求不高时, 提高求解效率. 使用时可以根据需要修改重正交化次数
   在精度较高时, W与XP正交化后, W自身的内积会小于1e-16, 小于机器精度, 此时调用lapack求解特征值问题会报错,
   因此这里添加了判断, 如果内积均小于机器精度(这里用orth_zero_tol), 那就认为W均为0, 这样计算不会出错, 但精度无法继续下降
   1) 修改了gcge_orthogonal.c
   2) 修改了gcge_orthogonal.h
3. 修改了GCGE_ComputeSubspaceMatrix中子空间计算的部分
   1) 修改了gcge_rayleighritz.c

2018.11.02 zhangning
1. 添加了正交化方式的选择
   1) 修改了gcge_para.h
      在GCGE_PARA中添加了w_orth_type, 用于选择使用哪种大规模正交化方式
   2) 修改了gcge_para.c
      添加了w_orth_type的默认值, 以及该参数的命令行参数设置方式
   3) 修改了gcge_eigsol.c
      添加了while循环中W正交化时对正交化方式的选择判断
2. 修改了GCGE_CBOrthogonal的W自身正交化时的重正交化次数(由para->max_reorth_time改为1)

2018.11.02-2  zhangning
1. 修改了求解子空间特征值的函数GCGE_ComputeSubspaceEigenpairs
   1) 修改了gcge_rayleighritz.c
      修改了函数GCGE_ComputeSubspaceEigenpairs(如果要关闭本次修改,只需要把这个子函数中rr_eigen_start赋值为0)
   2) 修改了gcge_orthotonal.c
      GCGE_ComputeSubspaceEigenpairs需要使用GCGE_OrthogonalSubspace,需要子空间正交化中添加一个参数
      在GCGE_OrthogonalSubspace中添加了一个形式参数, 算法过程中对使用到这个形式参数的地方做了修改
   3) 修改了gcge_orthotonal.h
      在GCGE_OrthogonalSubspace中添加了一个形式参数
   4) 修改了gcge_xpw.c
      GCGE_ComputeP中用到了GCGE_OrthogonalSubspace, 因为形式参数改变, 对这里做了修改
   5) GCGE_ComputeSubspaceEigenpairs中有计算子空间特征值和子空间正交化两部分, 因此对时间统计做了调整
      (1) 修改了 gcge_statistics.h
          在GCGE_PART_TIME中添加了x_orth_time 
      (2) 修改了 gcge_statistics.c
          添加了 x_orth_time 初始化为 0.0
      (3) 修改了 gcge_rayleighritz.c
          添加了rr_eigen_time和x_orth_time的时间统计
      (4) 修改了 gcge_eigsol.c
          删除了对rr_eigen_time的时间统计

2018.11.02-3  zhangning
1. 修改了线性组合用到的基底向量和计算得到的向量
   1) 修改了gcge_workspace.h
      添加了 num_soft_locked_in_last_iter 这个参数
   2) 修改了gcge_workspace.c
      添加了 num_soft_locked_in_last_iter 这个参数的初始化
   3) 修改了gcge_eigsol.c
      在GCGE_CheckConvergence中添加了更新 num_soft_locked_in_last_iter 的方式
      在GCGE_EigenSolver中修改了 MultiVecSwap 的向量起始位置
   4) 修改了gcge_xpw.c
      在GCGE_ComputeX中修改了线性组合的基向量V与结果向量X的起始位置, 以及线性组合系数的起始位置
      在GCGE_ComputeP中修改了子空间正交化的起始位置与矩阵长度, 线性组合时基向量V的起始位置, 以及线性组合系数的起始位置
   5) 修改了gcge_app_slepc.c
      修改了子函数GCGE_SLEPC_MultiVecLinearComb中线性组合系数的长度

2018.11.03-1 liyu
1. HYPRE在github上有multivector的结构和较为完整的操作支持, 准备对hypre再写一个接口, 称为mvhypre, 正如slepc一样
2. hypre中mv对线性解法器的支持不够, 如果mvhypre提供外部解法器时, 依然做循环求解. 

2018.11.03-2 liyu
关于HYPRE的文件结构, 以krylov文件夹中的pcg为例:
1. pcg.c pcg.h 是对pcg的实现, 它是矩阵向量FREE的实现, 通过pcg_data和pcg_functions管理数据与操作
   函数中的命名规则是hypre_xxx
   pcg.h保证pcg.c可以编译
2. HYPRE_pcg.c是对pcg.c中各类被外部调用函数的封装, 形式参数已经不是void*
   函数中的命名规则是HYPRE_xxx
3. krylov.h保证HYPRE_pcg.c可以编译即可.
4. krylov/中有直接生成.a文件, 所以可以直接链接它而非全部HYPRE的库文件

也就是说, krylov文件夹中所实现的完全是"空对空". 
HYPRE自己是在parcsr_ls(并行csr矩阵的linear solver)中再次通过
HYPRE_parcsr_pcg.c利用parcsr_mv中的矩阵向量结构和操作真正实现krylov中各种数据结构和操作
HYPRE_parcsr_ls.h是可以被外部调用的头文件, 包含所有线性解法器
_hypre_parcsr_ls.h是非常内部的头文件


我们再看HYPRE的文件结构
# 配置安装用, 生成的.a.h在hypre/中用以被外部程序调用
cmbuild
config
hypre
# 文档, 例子和测试程序
docs
examples
test
# 重写的blas和lapck, 命名规则为hypre_xxx
blas
lapack
# 工具箱, 管理内存等
utilities
# 底层各类矩阵向量结构
seq_mv
matrix_matrix
multivector
parcsr_block_mv
# 高级矩阵向量结构封装
IJ_mv  
FEI_mv
# 线性解法器(空对空)
krylov
# 各类矩阵向量结构, 直接有对应的线性解法器接口(当前版本)
parcsr_mv
struct_mv
sstruct_mv
distributed_matrix
# 对应的线性解法器接口
parcsr_ls  (krylov和AMG)
struct_ls
sstruct_ls
distributed_ls


文件命名规则
xxx一般是当前文件夹的名字
_hypre_xxx.h非常内部的头文件，会包含HYPRE下其它文件夹下的头文件
HYPRE_xxx.h一般会被_hypre_xxx.h包含(include), 更高一层的头文件, 可被外部程序(include)

函数命名规则
hypre_是内部调用, HYPRE_是外部调用


2018.11.03-3 liyu
如果需要利用multivector中的多向量结构进行线性方程组的求解, 那么只需要重写一下
parcsr_ls/HYPRE_parcsr_pcg.c可以起名为multivector_ls/HYPRE_multivector_pcg.c
也就是说, 非常容易实现块的各类krylov线性解法器
注意的是lobpcg的实现稍微有点不一样, 它是通过multivector中的interpreter实现的


2018.11.03-4 liyu
关于AMG的multivector实现
1. AMG是依赖于矩阵结构的所以只能出现在parcsr_ls中, 如果要将它改成multivector的情形, 只能重写parcsr_ls中的amg部分
   还有一种方式就是直接将矩阵的分层矩阵和插值矩阵拿出来, 然后利用multivector的操作调用krylov进而实现multivector_amg
2. 换一种思路, 如果循环多向量中的每个单向量, 然后调用AMG且迭代次数很小, 然后再回到已经容易实现的multivector_krylov
   这种方法的思想预条件子的构造总是要涉及到矩阵的具体结构, 而且预条件问题本就是不用迭代很多次.
   不算预条件子生成时间的话, 求解预条件矩阵对应的线性方程组应该不是计算的主要时间. 

2018.11.03-1 zhangning
1. 修改了gcge_cg.c
   修改了GCGE_BCG, 取出向量组的单向量时, 最多只能取两个(BV结构的要求)


2018.11.04-1 liyu

使用MKL包

1. 首先运行(可添加在.bashrc)
. /share/soft/intel_2018_update1/bin/compilervars.sh intel64
   这样就配置好了icc ifort mpiicc mpiifort mkl
   通过which icc检查是否正确
2. 链接时除了需要链接对应的在xxx/mkl/lib/intel64下的库文件之外, 
   链接的FLAG需要加入-mkl=cluster 详见Chapter 3
3. 利用bsub提交程序的时候, 一定用mpiexec, 其它都是脚本

1 mpirun=/share/soft/intel_2018_update1/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/mpiexec.hydra
2 for nprocs in 36 72 144 288 576 1152
3 do
4   bsub -J ex11 -q big -R "span[ptile=36]" -n $nprocs -e test.err -o test.out \
5      "$mpirun -np $nprocs -ppn 36 ./ex11 -n 10000 -blockSize 10"
6 done

lp64和ilp64只有在整型上有区别


2018.11.04-1 zhangning

1. 编译时有warning, 在一些头文件中增加了一些#include "gcge_statistics.h"与#include "gcge_utilities.h"
2. 修改了gcge_eigsol.c中的printf为GCGE_Printf, 因printf会在并行时打印多次
3. 删除了gcge_para.c中打印信息时的一个\n换行符

2018.11.04-2 zhangning

1. 添加了RayleighRitz中对于特征值重数的判定(计算子空间特征值问题时, 只计算未收敛的第一个到max_dim_x, 
   但如果未收敛的第一个特征值是重特征值, 可能会出问题)
   1) 在 gcge_para.h, gcge_para.c 中添加了判断eval[unlock[0]]重数的参数 multi_tol_for_lock, 默认值为 1e-6
   2) 原本在gcge_eigsol.c中更新的 num_soft_locked_in_last_iter, 改到 ComputeSubspaceEigenPairs 时更新
   3) 在 gcge_rayleighritz.c 中 GCGE_ComputeSubspaceEigenpairs 函数中添加重数的判定
2. 添加了 如果需要在求解特征值问题后对特征向量进行广播, 那就只计算 0 号进程的特征值问题, 然后将 0 号进程的
   特征对广播到各个进程
   1) 修改了 gcge_orthogonal.c, 因为用到了memcpy, 所以在gcge_orthogonal.h 中添加了#include<string.h>
   1) 修改了 gcge_rayleighritz.c, 因为用到了memcpy, 所以在gcge_rayleighritz.h 中添加了#include<string.h>
3. 添加了 Omega范数 这种收敛准则, 这种收敛准则需要用到一个参数conv_omega_norm, 用于存储矩阵A的Omega范数
   1) 修改了gcge_para.h
      添加了conv_omega_norm, 并修改了conv_type的注释
   2) 修改了gcge_para.c
      添加了conv_type的默认值, 并修改了GCGE_PrintParaInfo
   3) 修改了gcge_eigsol.c
      为了使用Omega范数这种收敛准则, 首先用随机初值计算了这个范数, 然后在判断收敛性时使用了这个范数


2018.11.04-3 zhangning
1. 在ComputeW中, 如果提供了外部求解器, 那在外部求解器解完后再调用BCG求解, 如果不用BCG, 
   就将cg_max_it设置为0即可

2018.11.05-1 zhangning
使用petsc外部求解器并使用hypre的boomeramg求解预条件矩阵方程组的一些说明
1. 如果使用外部线性求解器, 在GCGE_ComputeW中把每一个线性方程组的解向量和右端项取出, 
    调用ops->LinearSolver求解, 再统一调用BCG迭代统一求解所有的线性方程组
2. 调用ops->LinearSolver求解过程中, 以使用petsc的cg迭代为例, 每次cg迭代中, 都要解一次预条件矩阵线性方程组.
    以使用hypre的boomeramg作为预条件方法为例, 在PCSetUp时将petsc格式的预条件矩阵赋给hypre矩阵.
    将petsc格式的解向量与右端项赋给hypre向量.

2018.11.05-1 liyu
1. OPENMP要慎用, 除非节点数非常多, bsub如何进行MPI+Openmp还没有搞明白, mpijob这个脚本需要再学习
2. 使用openmp之后利用bsub提交MPI作业是也出现问题, 所以HYPRE PETSC SLEPC都不再包含openmp
3. 使用amg+BCG好像不会对问题有任何改进, 也就是说, 精确求解W是没什么意义的.
4. 使用-gcge_block_size可以大幅提升运行效率, 一般取为nev/4
5. 在计算特征值非常多的情况下, gcge比lobpcg好很多, 而特征值较少时, 两者性能相当

2018.11.06-1 zhangning
1. 添加了参数 opt_rr_eig_partly, 表示是否做只计算RR中未收敛特征对的优化，默认为1
   添加了参数 opt_bcast, 表示是否做广播前只用0号进程计算子空间特征值问题的优化，默认为1
   1) 修改了 gcge_para.h gcge_para.c
   2) 修改了 gcge_rayleighritz.c 添加了对这两个参数的使用
   3) 修改了 gcge_orthogonal.c 添加了对 opt_bcast 的使用
2. 修改了 gcge_xpw.c ,
    1) 因为用外部求解器后再调用BCG没有好的结果，所以注释掉这里的BCG
    2) 添加了使用普通cg还是BCG的判断,添加了参数if_use_bcg,这里也修改了gcge_para.c gcge_para.h
3. 修改了gcge_cg.c中普通cg中对RitzVec的使用
4. 添加了test/slepc中的三个具体的测试文件, 修改了test/slepc中的Makefile
5. 添加了test/petsc中的两个具体的测试文件, 修改了test/petsc中的Makefile

2018.11.07-1 zhangning
1. 添加了一种新的正交化方式 GCGE_SCBOrthogonal
    这种正交化方式相比GCGE_CBOrthogonal更稳定，且比GCGE_BOrthogonal更高效，是最推荐的正交化方式
    目前默认的正交化方式仍为最稳定的GCGE_BOrthogonal
    要使用GCGE_SCBOrthogonal的话需要设置-gcge_w_orth_type scbgs，或者 para->w_orth_type="scbgs";
2. 将block_size默认值设为 nev/5 与 1 的最大值
3. 将w_orth_type默认值设为 scbgs

2018.11.08-1 zhangning
1. 给orth_zero_tol加了DBL_EPSILON
    1) 在gcge_type.h中添加了#include<float.h>
    2) 在gcge_para.c中的Setup中添加了+=DBL_EPSILON
2. 添加了参数scbgs_wself_max_reorth_time
    1) 修改了gcge_para.h gcge_para.c
    2) 在gcge_orthogonal.c 中使用它

2018.11.09-1 zhangning
1. 添加了两种子空间正交化方式：GCGE_BOrthogonalSubspace与GCGE_SCBOrthogonalSubspace,
    并添加了选择子空间正交化方式的参数p_orth_type
    1) 修改了gcge_para.c gcge_para.h，添加参数p_orth_type
    2) 修改了gcge_orthogonal.c gcge_orthogonal.h，添加了两种子空间正交化函数
    3) 修改了gcge_xpw.c gcge_rayleighritz.c，添加了其中对于子空间正交化方式的选择
2. 继续添加了X部分子空间正交化方式的参数x_orth_type
    1) 修改了gcge_para.c gcge_para.h，添加参数x_orth_type
    2) 修改了gcge_rayleighritz.c，修改了其中对于X部分子空间正交化方式的选择


2018.11.09-1 liyu
修改了整体的文件结构发布2.0版本，且自适应编译代码
将blas lapack嵌入软件包, 如果用户未给确定的LIBBLAS LIBLAPACK
对于MPI的适应是修改gcge_type, 通过判断编译器CC是否包含mpi这个字段

TODO:
可以在Makefile中的install里，去判断install_libs而不是去判断文件是否存在?

2018.11.10-1 zhangning
1. 添加了分批计算的子空间正交化方式，并在计算X部分的子空间正交化时使用
    1) 修改了gcge_orthogonal.c gcge_orthogonal.h添加了GCGE_BlockOrthogonalSubspace
    2) 修改了gcge_para.h gcge_para.c 添加了正交化分批计算的x_orth_block_size这个参数
    3) 修改了gcge_rayleighritz.c 添加了这种正交化方式的调用
2. 修改了SCBOrthogonal为分批计算的方式，并在W部分的正交化时使用
    1) 修改了gcge_orthogonal.c gcge_orthogonal.h
3. add a parameter orth_para->w_orth_block_size in GCGE_PARA
    1) 修改了gcge_para.h gcge_para.c 添加了正交化分批计算的w_orth_block_size这个参数
    2) 修改了gcge_orthogonal.c

2018.11.11 liyu
1. 将phg加入app和test, 由于自适应加密部分不能被并行调用，所以只是读网格，
   然后生成矩阵，还没有完全做的是，如何将VEC转化成DOF
2. 将make.inc中PETSC, SLEPC, PHG需要链接的库文件显示写出来了，这样比较直观

2018.11.12 liyu
TODO
1. app可以不用分文件夹, 每个app只有一个.c一个.h，所以放在一起也可以
2. examples需要将test中的最简单的一个程序放进来，原则是编译好后，用户可以直接利用命名行参数进行调用
3. examples写好后就可以在install中将上述生成的可执行文件加到bin中
4. 在LSSC4上安装SuperLU以及直接求解器，比如UMFPACK以及相应软件，重新编译各种app对应的软件
5. 对于test文件，需要对测试内容标准化，首先从维护csr接口开始
6. README.md写得非常差，还需要再改进
7. Scalapack需要加入使得小规模特征值问题可以快速求解

2018.11.12-1 zhangning
1. 把dev分支上的更新加入到dev-2.0
    1) 修改了 gcge_para.c gcge_para.h gcge_orthogonal.c gcge_orthogonal.h gcge_xpw.c gcge_rayleighritz.c
   见 log 2018.11.09-1 zhangning 与 2018.11.10-2 zhangning

2018.11.12-2 zhangning
1. gcge_para.c中发现一个bug, GCGE_PARA_Setup中w_orth_block_size没能正确更新
    1）修改了gcge_para.c

2018.11.12 liyu
需要做的基本测试:(不仅测试正确性, 还要测试其并行效率, 如果接口是并行的话. 下述输入也可以是BV结构的输入)
1. 矩阵向量操作     y = a Ax + b y, y = a x + b y, res = x'y                 a, b, c是常数，A是稀疏矩阵, x, y, z是向量
2. 矩阵多向量操作   y = a Ax + b y, y = a x + b y, res = x'y                 a, b, c是常数，A是稀疏矩阵, x, y, z是多向量
                    y = x a (linear combination)                             a是稠密矩阵, x, y是多向量
3. 正交化           orth(A, x, y) (给定对称正定矩阵A，关于x对y做正交化)      A是稀疏矩阵，x, y是多向量
4. 线性求解器       linear_solver(A, b, x)                                   A是稀疏矩阵，x, b是多向量 

需要做的算法测试:
1. computeX, computeP, computeW
2. 在给定X和A, B时，计算RR过程，从生成小规模矩阵到求解R值。这里测试各种分批计算以及部分计算R值的效果
3. 不同接口下命令行参数的使用

需要修改的核心代码:
1. workspace部分一定要有自己的管理函数
2. computeX, computeP, computeW, orth, linear_solver一定要能分的开, 换句话讲, 可以不受其它影响下进行测试
   更重要一点是, 它们的形式参数不能包含workspace, 只能包含它的具体成员，使得每个部分都是局部影响的
3. 核心函数的调用一定要写成像LAPACK一样的形式参数, 或者说像lobpcg一样的也就是说，
   核心函数的实现只用形式参数提供的操作接口:
   矩阵多向量操作, 正交化, 线性解法器, 求解小规模特征值问题
   核心函数的操作空间调用workspace的具体成员
4. 每个部件都可以被优化, 整体算法应该就足够优化, 但也需要从整体上寻找需要做全局优化的地方
5. SOLVER这个结构体的各类操作以及普通用户的简单接口需要再斟酌

2018.11.24 zhangning
1. gcge_para.c中Setup时，对w_orth_block_size的更新有bug, 这个参数必须大于0，做了修改
2. gcge_para.c中Setup时，对block_size的更新改为 (nev/5 > 1)?(nev/5):nev;
3. gcge_orthgonal.c中，BlockOrthogonalSubspace中，对orth_block_size的设置做了修改，确保它一定>0
//4. test/slepc/中添加了更一般的测试函数test_slepc_solver.c

2018.12.03 liyu
1. 安装MUMPS, 对于petsc和phg, 我将s和d的版本都放在了一起, libmumps.a
2. 修改了make.inc中的petsc, slepc, phg的LIB
3. 修改了test/petsc中的makefile, 不完全
4. 但是petsc和phg的所用的mumps貌似不是一个版本, 前者是d后者s

2018.12.04 liyu
1. gcge_ops.c 修改多向量内积 
   首先，做局部内积, 然后基于MPI_Type_vector将局部结果生成一个新的数据结构;
   然后，利用MPI_Op_create自定义一个操作, 使得新的数据结构可以进行Allreduce求和
   最后, 释放新定义的结构体和操作
2. TODO:其它涉及频繁全局通信的地方, 都可以这样处理

2018.12.05 zhangning
1. gcge_rayleighritz.c 修改子空间特征值问题求解函数 GCGE_ComputeSubspaceEigenpairs
   将子空间特征值平均分配给每个进程进行计算, 计算结束后再做MPI_Allgatherv统一到所有进程
   添加了参数opt_allgatherv，默认为1，可以命令行选择是否使用这种优化
   修改了 gcge_rayleighritz.c gcge_para.c gcge_para.h

2018.12.06 zhangning
1. TODO: gcge_para.c 中 opt_allgatherv默认值改为0，allgatherv后有时需要再对X部分做一次正交化，要再考虑
2. 添加multi正交化
    修改了 gcge_orthogonal.c .h
    修改了 gcge_eigsol.c gcge_xpw.c
    修改了 gcge_para.c .h

2018.12.07 zhangning
1. 修改了 gcge_para.c，将正交化的默认方式改为 multi
//2. 将子空间opt_allgatherv部分恢复成原来的方式
3. opt_allgatherv已经改回最新的方式
4. 重新添加了 given_init_evec这个参数，默认用户不提供初值
5. 添加了对初始X进行正交化的时间统计

2018.12.09 zhangning
1. multi 正交化中，减去前面分量时，如果前面已经是空的，就不需要再减。加判断，前面是否是空的
2. gcge_eigsol.c 之前没有把V中特征向量赋给evec，做了这里的修改

2018.12.10 liyu
1. 利用phg中的refine.o和coarse.o代替开源版本, 使得LSSC4上的phg可以做并行自适应加密放粗以及负载平衡
2. 对test/phg中的测试函数, 利用几何多重网格, 将特征值问题从粗空间一步步算到细空间, 这里默认是直接二分加密三次.
   也可以进行自适应加密, 但对于特征值问题, 这个没必要.
   测试的结果是当有足够精度时, 在每个细空间GCGE只进行两次迭代, 第一次是对X正交化并求解rr问题;
   第二次是计算W，并做[X W]的rr问题. 这样就可以达到精度的要求。
3. TODO 在测试过程中发现几个bug已由上一条log记录.但是对于block_size的选取还是有些问题
   希望, 必须是同一个特征子空间的特征向量要一起收敛才算收敛, 如果只是一个收敛, 且它后面有重根, 
   在下一次迭代后也要将这个特征向量再更新, 尤其对于部分计算rr值的情况。
   另外就是默认参数最好是都算rr值, 比较稳定

2018.12.11 liyu
TODO:
1. 由于一般的软件包都不支持多向量操作, 所以可以尝试修改CreateMulitVec, 使得连续存储多向量中的data部分.
   那么, 需要在ops中新加入两个成员函数
   void (GetDataFromVec)           (void *vec, GCGE_INT *size, GCGE_DOUBLE **x);
   void (RestoreDataForVec)        (void *vec, GCGE_INT *size, GCGE_DOUBLE **x);
   这样在创建多向量的时候, 都是先创建再释放data部分, 然后再给其赋值到已经生成的位置
   比如根据当前多向量data的size之和, 生成一个整块, 然后将对应的地址赋给data
   当然, 这里需要注意的是, 当进行multivec的Destroy时, 需要确认如果data是NULL是否会报错. 
   另外, 就是需要自己有一个结构去管理这些多向量结构的数据
2. 将PASE直接嵌入GCGE是一个很好的思路, 首先在GCGE的多向量操作下, 写AUX多向量操作
   然后再增加一个MG结构, 使得PASE可以在GCGE框架下实现
3. 编写HYPRE与PETSC矩阵之间的转换函数, 以及各种app的向量与SLEPC中BV的转化函数
4. 通过3的过程以及新增MG结构, 就可以写块的AMG求解线性方程组
5. 当前版本的PHG接口可以实现几何多重网格计算特征值，在一致加密的情况下, GCGE需要迭代2次.
   在自适应加密的情况下, 需要迭代1次. 之所以是1次, 应该是网格对解变化大的地方做了更多的加密.
   那么可以考虑自适应加密之后再一致加密一次, 使得迭代做两次
6. 一致加密GCGE迭代2次收敛的理论分析比较像是2网格的思路, 在粗空间解特征值问题, 在细空间解解问题之后, 
   依然计算特征值问题, 当有一定精度的时候, 再继续到下一层. 这样就是二网格
   更重要的是W的计算有非常好的初值，相当于是AMG中的光滑.
7. 对于CSR在MPI下的运行, 是否可以将CSR包在MPI下就不再编译了，当然可以在它的测试函数中加入
#if GCGE_USE_MPI
   MPI_Init(&argc,  &argv);
#endif
   ...
#if GCGE_USE_MPI
   MPI_Finalize();
#endif
8. PHG的默认特征值求解器是什么呢? 在LSSC4上管理员安装的PHG包含了所有的外部包吗?

2018.12.11 zhangning
1. CG和BCG不依赖于GCGE_WORKSPACE
   修改 gcge_cg.c gcge_cg.h gcge_xpw.c
2. 修改 test_slepc_solver.c 如果不给输入矩阵，就算差分矩阵标准特征值问题

2018.12.12 zhangning
TODO
1. 子空间矩阵计算特征值时，往前考虑重数，但此时矩阵没有把重数的部分构造出来，而是在判断重数之后多构造的。
   应该在构造矩阵前先重新判断当前的重数，即把rr_eigen里面判断重数的部分挪到构造矩阵之前

2018.12.15 zhangning
1. 修改了gcge_ops.h中MultiVecLinearComb的参数, 进而修改了相关的 gcge_ops.c gcge_xpw.c gcge_orthogonal.c中的调用格式
   去掉了 void* dmat, GCGE_INT lddmat, 添加了 GCGE_INT if_Vec, GCGE_DOUBLE alpha, GCGE_DOUBLE beta
2. 修改了gcge_ops.h中MultiVecInnerProd的参数，进而修改了相关的 gcge_ops.c, gcge_rayleighritz.c gcge_orthogonal.c中的调用格式
   添加了 GCGE_INT if_Vec

2018.12.16 zhangning
1. pase文件夹中，修改了pase_ops.c pase_ops.h pase_matvec.c pase_matvec.h
2. test/pase文件夹中，添加了test_ops.c，测试了CSR格式下的pase_ops，均测试通过
3. 在make.inc(3个文件)中添加了 GCGEPASEINC与LIBGCGEPASE

2018.12.17 zhangning
1. 测试时发现GCGE_DefaultMultiVecAxpbyColumn的一个bug，之前因为没有用到这个函数，所以没检查到错误
   错误原因在于对BV向量组结构使用了x[1]这种操作
   in gcge_ops.c
2. test/pase文件夹中，添加了test_slepc_pase_ops.c, 其测试是在example中进行的

2018.12.18 zhangning
1. GCGE_OPS 结构体的函数中都添加了 GCGE_OPS *ops这个形式参数，用于PASE接口使用
2. 修改了GCGE/Makefile，添加了pase的内容

2018.12.19 zhangning
1. pase接口一个特征值时可以使用, 需要对多个的情形再调试
2. 修改了pase_ops.c中一些错误的内容，修改了test_ops.c,另附matlab测试程序，使测试更方便
3. 修改了DenseMatEigenSolver的调用方式，简化了参数

2018.12.19 liyu
1. 这pase文件夹下对pase_convert.c进行测试，临时将测试函数放在pase文件夹下
2. 测试了从PETSC矩阵到HYPRE矩阵以及AMG生成一系列HYPRE矩阵, 再转化为PETSC矩阵，然后生成对应一系列向量
   并且做了完备的矩阵向量测试，以待后期使用

2018.12.24 zhangning
1. 修改了GCGE_BCG的参数，使得不依赖于GCGE_PARA，可以在pase中调用
2. 目前 test/pase/test_pase_1.c 是一个简单的二网格pase测试例子，可以运行，
没有内存错误，但是结果有问题，使用的是CSR格式
   1）复合矩阵构造没问题，对确定的限制矩阵R，近似特征向量x，粗细矩阵Af,Ac,与matlab对比过，构造的矩阵
      [Ac,        R*Af*x;
       x'*Af'*R', x'*Af*x]
      是相同的
   2）对上述复合矩阵，用GCGE-PASE求解的特征向量是没问题的
   3）但这样使用GCGE求解后，残差反而比初始的x变大

2018.12.25 liyu
1. 需要在OPS中填加矩阵转置乘以向量的操作.
2. MultiGrid结构中[ABP]_array都是重新生成的Mat型数组, 应该与传入的A和B相同
3. 还未测试MultiGrid

2018.12.25 zhangning
1. GCGE_OPS中添加了MatTransposeDotVec
                   MatTransposeDotMultiVec
   修改了 gcge_app_slepc.c gcge_app_petsc.c gcge_ops.c gcge_ops.h

2018.12.27 zhangning
1. 修改了test_mg.c pase_mg.c
   两层的Multigrid测试没有问题
2. 修改了test_mg.c pase_mg.c pase_mg.h
   三层的Multigrid测试没有问题

TODO
怎么确认分层是对的？经测试，A_array[0]与A_array[1]的特征值相差很多
test_mg.c中有打印矩阵的函数，打印后可以直接粘贴到matlab中读入

2018.12.30 zhangning
1. pase_amg.c.h 块AMG，在test_amg.c测试通过

2019.01.01 zhangning
1. gcge_cg.c 修改了 lock 相关的内容
2. pase_mg.c 修改了 FromItoJ 的内容
3. pase_solver.c 可以正确运行
4. 修改了测试文件 test_amg.c test_pase.c

2019.01.03 zhangning
1. 修改了pase_solver.h, pase_param.h中的结构，重写了pase_solver.c以及pase_param.c,可以运行通过，结果正确
2. 相应的修改了pase_mg.c pase_mg.h pase_amg.h以及测试文件test_pase.c
TODO
3. 用一个进程，set_up时间占比非常高，用8个进程，set_up时间就非常少，目前不清楚原因
4. 迭代次数比原有的多, 要对比效率

2019.01.08 zhangning
1. hhxie修改了gcge_cg.c
2. 修改了pase中的一些参数

2019.01.10 zhangning
1. 修改了组装aux矩阵，对已经收敛并且lock住的部分不再重复计算
   并为此修改了BMG和BCG中对于几个临时向量组空间的使用

2019.01.21 liyu
1. 加入PHG矩阵转化为HYPRE矩阵
2. 加入PHG矩阵转化为PETSC矩阵, 测试未成功，原因是PHG和PETSC有对相同MPI函数的重新定义，也可能是版本问题
   但是mg结构是直接用PETSC矩阵的，所以，如果需要用PHG做原始矩阵的话，还需更改MultiGridCreate
----------------------------------------------
   不确定PHG和PETSC到底是否可以混合一起用
----------------------------------------------
3. 总结
   1. AMG做粗化得到的粗矩阵,  在每个进程上的存储的行数并不要求,  也就是说,  有些进程可以包含这个并行矩阵的0行.  
      那么,  并行矩阵乘以向量是如何进行的呢?
   2. 再查看矩阵乘以向量的源码发现,  它并不直接调用MPI的程序,  
      它封装了MPI的SendRev,  使得适用于某个进程无需通讯的问题. 这又是如何做到的呢?
   3. 每个矩阵都有一个成员CommPkg,  其实就是包含通讯域的一个结构体,  以及与SendRev相关的信息.
   
   1. src/parcsr_ls/par_amg_setup.c
   2. src/parcsr_mv/par_csr_matvec.c
   3. src/parcsr_mv/par_csr_communication.h

2019.01.21 zhangning
添加了求解模最小特征值的功能
1. gcge_para.c.h中添加了eval_type参数，如果是sa则求值最小，sm则求模最小
2. 修改了gcge_rayleighritz.c.h以处理模最小的情况
3. 修改了gcge_workspace.c中subspace_evec的空间大小以适应模最小的情况
4. 修改了test_slepc_solver.c，添加了位移的处理方法
5. 修改了gcge_app_slepc.c添加了minres的配置方法

2019.01.27 zhangning
pase_solver中添加了对aux_sol的排序，并将cg_double_tmp大小修改为max_nev*max_nev
1. 修改了pase_solver.c pase_solver.h

2019.12.25 liyu
1. 将pase与gcge剥离，只放在gcge的主文件下，需要进入之后单独make libs进行编译
2. pase_ops.c中，修改425行的关于线性组合的小bug
3. 整理pase中的.h文件
4. 可以考虑将pase_mg和pase_amg并入gcge中，并且在gcge_ops中加入新的操作，比如ItoJ, CreateMGMatrixes
5. 去掉PASE_MULTIGRID_Create形式参数中的pase_ops

2019.12.27 liyu
1. 修改gcge_eigsol.c 760行 判断收敛是当idx==0时，不能访问idx-1
2. 修改pase_amg.c中，BCG的调用，都改为调用BCG_Continuous
3. 未修改pase_mg.c中，FromItoJ, 这里用了为做初始化的MULTIGRID的结构成员，而且if else也非常奇怪

2019.12.30 liyu
1. 修改gcge_app_slepc.c中，SetRandomValue, 使得每次调用这个操作时，会有不同的随机数生成
   相应的gcge_app_petsc.c中也做相应修改
2. 修改gcge_app_hypre.c中，hypre 15中，对于hypre_CTAlloc增加第三个形式变量，添加宏 HYPRE_MEMORY_HOST
3. BCG和BMG的收敛准则比较奇怪，首先，如果BCG的初值就是精确值，则会出错。BMG的rate不要太小
4. PASE_EigenSolver对于一些边角情况没有任何处理，比如，多层网格情形下，粗矩阵的阶数可能小于特征值数
5. convert部分还没有完全检测完成

2020.01.01 liyu
1. 在pase/test中的get_mat_phg.c可以直接调用phg得到刚度矩阵A和质量矩阵B
2. 在pase/test中的test_mg_phg.c测试利用CreateMatrixPHG生成phg的矩阵，并将之转化为PETSC矩阵，这里利用void型进行操作，不会让petsc和phg同框出现
3. 在pase/example中pase_slepc_solver.c没有写好，有问题
4. 在pase/src中pase_solver.c，关于aux sol sort部分有问题，主要是做内积时multigrid的成员double_tmp不够长
5. 多个软件包混用的时候，会有一个内存错误，查不出原因，不过不影响结果
6. phg关于处理边界条件和map的函数有内存错误

2020.01.03 liyu
1. 在pase/src/pase_mg.c中，利用petsc自带的amg去生成分层矩阵 (以做测试 pase/test/test_mg.c)
   A[0] is the refinest matrix and A[1] == P[0]^T A[0] P[0]; 
   在petsc的amg中,  nrows of A[0] / nrows of A[1] ，这个比率显著大于hypre's 
2. 总结关于PCGAMG的设置函数, 具体参看
https://www.mcs.anl.gov/petsc/petsc-current/docs/manualpages/PC/index.html
a. Set maximum number of equations on coarsest grid.
   PetscErrorCode PCGAMGSetCoarseEqLim(PC pc,  PetscInt n)
b. Relative threshold to use for dropping edges in aggregation graph Synopsis
   PetscErrorCode PCGAMGSetThreshold(PC pc,  PetscReal v[],  PetscInt n)
c. allow a parallel coarse grid solver
   PetscErrorCode PCGAMGSetUseParallelCoarseGridSolve(PC pc,  PetscBool flg)
d. Set number of equations to aim for per process on the coarse grids via processor reduction.
   PetscErrorCode  PCGAMGSetProcEqLim(PC pc,  PetscInt n)
e. Repartition the degrees of freedom across the processors on the coarser grids
   PetscErrorCode PCGAMGSetRepartition(PC pc,  PetscBool n)

2020.01.14 zhangning
1. 在pase/test中添加了test_mg_phg_petsc.c与test_mg_phg_petsc_ksp.c，分别用BMG和KSP求解，同时在Makefile添加相应的编译路径
2. pase/test/get_mat_phg.c与test_mg_phg.c没有修改，但git status时显示modified
3. pase/src/pase_solver.c第300行改为size_dtmp = max_nev * step_size;
4. 在src/gcge_ops.h中添加MultiLinearSolver
5. 在src/gcge_ops.c初始化时添加(*ops)->MultiLinearSolver = NULL;
6. 在src/gcge_xpw.c中GetW时添加if(ops->MultiLinearSolver)多向量LinearSolver调用方式
7. 在pase/src/pase_mg.c PASE_MULTIGRID_Create中添加对最粗层层号的判断，避免coarsest_level超过最粗层

2020.02.23 zhangning liyu
1. src/gcge_cg.c中，GCGE_BCG函数中:
   1) 在while循环前加入是否收敛的判定
   2) while循环的最后，判定是否收敛时，相对误差由除变乘，另外增加绝对误差的判定
   3) BCG对应的rate还是有一点问题, 现在修改了收敛准则，减小最大迭代次数，可以运行，但迭代次数大时仍有问题，需进一步测试
2. 利用petsc的GAMG进行各种设定时, 一定用 PCGAMGSet...的函数进行.

2020.03.10 zhangning liyu
下述讨论可能出现的问题
1. 多算的特征值个数,  workspace->max_dim_x = nev+20
2. 收敛准则需要同时判断绝对残差的大小 现在设为1e-1
3. 新生成的向量, 如果长度接近于0, 则去掉这个向量, 这里存在一个参数, 不能太小
4. 当利用phg模块时, 传递命令行参数 需要 ./a.out -oem_options "-your_option 3"

2020.03.14 liyu
1. 利用phg并且需要输入命令行参数时, 对于需要输入gcge的命令行参数时, 需要利用
-oem_options "-gcge_nev 10 -gcge_tol 1e-5"
并且在main.c中需要加入一段代码, 将"..."看空格分开, 并生成一个argc_new argv_new
{
    char *str, c;
    int i, k, flag;
    int argc_new = 0;
    char **argv_new = malloc(16*sizeof(char*));
    for (i=0; i < 16; ++i)
    {
       argv_new[i] = malloc(64*sizeof(char));
    }
    i = 0;
    while (i < argc)
    {
       if(0 == strcmp(argv[i], "-oem_options"))
       {
	  ++i;
	  str = argv[i];
	  argc_new = -1;
	  flag = 0;
	  for(i=0;(c=str[i])!='\0';i++)
	  {
	     /* get the start of word */
	     if(c==' ')
	     {
		flag = 0;
	     }
	     else
	     {
		if (flag==0)
		{
		   flag = 1;
		   ++argc_new;
		   k = 0;
		}
		argv_new[argc_new][k++] = c;
		argv_new[argc_new][k] = '\0';
	     }
	  }
	  break;
       }
       else 
       {
	  ++i;
       }
    }
    argc_new += 1;
    printf("argc_new = %d\n", argc_new);
    for (i = 0; i < argc_new; ++i)
    {
       printf("%s ", argv_new[i]);
    }
    printf("\n");
    GCGE_INT error = GCGE_PARA_SetFromCommandLine(slepc_solver->para, argc_new, argv_new);
    for (i=0; i < 16; ++i)
    {
       free(argv_new[i]);
    }
    free(argv_new);
}

2. 利用phg并且需要输入命令行参数时, 对于需要输入slepc的命令行参数时, 需要利用
-oem_options -eps_nev 10 -oem_options -eps_tol 1e-5
需要一个个输入

3. 上述两点操作, 可参见 pase/test中的test_mg_phg_petsc.c test_phg_slepc.c 以及相应的提交脚本

2020.3.15 liyu
1. 关于查看petsc矩阵的信息
typedef struct {
   PetscLogDouble block_size;                         /* block size */
   PetscLogDouble nz_allocated, nz_used, nz_unneeded; /* number of nonzeros */
   PetscLogDouble memory;                             /* memory allocated */
   PetscLogDouble assemblies;                         /* number of matrix assemblies called */
   PetscLogDouble mallocs;                            /* number of mallocs during MatSetValues() */
   PetscLogDouble fill_ratio_given, fill_ratio_needed;/* fill ratio for LU/ILU */
   PetscLogDouble factor_mallocs;                     /* number of mallocs during factorization */
} MatInfo;
#include "petscmat.h" 
PetscErrorCode MatGetInfo(Mat mat, MatInfoType flag, MatInfo *info)


2020.3.20 liyu
typedef const char* BVType;
#define BVMAT        "mat"
#define BVSVEC       "svec"
#define BVVECS       "vecs"
#define BVCONTIGUOUS "contiguous"
#define BVTENSOR     "tensor"

利用-bv_type 修改BV的结构
eps默认的bv结构是SVEC, 不知道这些有什么区别
这个结构在创建时会计算global nrows of matrix * nev

2020.4.25 liyu
准备将gcge中添加amg, mg结构使得，代数多重网格以及BCG可以被其它程序引用
先不进行gcge的分离，即 分成两部分，运算与特征值算法
可以将MG的结构写在ops里，同时将app里，对于不同接口，写出不同的mg
ops里，关于mg，给出两个函数，一个是创建，一个是销毁
关于LinearSolver应该也是类似两个函数

2020.4.28 liyu
1. 在ops中添加
MultiGridCreate
MultiGridDestroy
VecFromItoJ
MultiVecFromItoJ
前两个在app中实现，后两个在gcge_ops.c中默认实现
2. 实现hypre, petsc, slepc的mg接口，可以考虑将以前我们自己的程序，针对csr矩阵也实现mg
3. hypre已经有multivec的结构，可能会比SLEPC要快
4. 以后对于gcge的修改原则是，先在其外进行实现，完全成功后再将之放在gcge中

2020.5.15 liyu
1. MatSetSizes
Sets the local and global sizes, and checks to determine compatibility Synopsis

#include "petscmat.h"
PetscErrorCode  MatSetSizes(Mat A, PetscInt m, PetscInt n, PetscInt M, PetscInt N)

Collective on Mat

Input Parameters
	A 	- the matrix
	m 	- number of local rows (or PETSC_DECIDE)
	n 	- number of local columns (or PETSC_DECIDE)
	M 	- number of global rows (or PETSC_DETERMINE)
	N 	- number of global columns (or PETSC_DETERMINE)

Notes
m (n) and M (N) cannot be both PETSC_DECIDE If one processor calls this with M (N) of PETSC_DECIDE then all processors must, otherwise the program will hang.

If PETSC_DECIDE is not used for the arguments 'm' and 'n', then the user must ensure that they are chosen to be compatible with the vectors. To do this, one first considers the matrix-vector product 'y = A x'. The 'm' that is used in the above routine must match the local size used in the vector creation routine VecCreateMPI() for 'y'. Likewise, the 'n' used must match that used as the local size in VecCreateMPI() for 'x'.

You cannot change the sizes once they have been set.

The sizes must be set before MatSetUp() or MatXXXSetPreallocation() is called.



2. MatPartitioningApply
Gets a partitioning for a matrix.
Synopsis

#include "petscmat.h" 
PetscErrorCode  MatPartitioningApply(MatPartitioning matp,IS *partitioning)

Collective on Mat

Input Parameters
matp -the matrix partitioning object

Output Parameters
partitioning -the partitioning. For each local node this tells the processor number that that node is assigned to.

Options Database Keys
To specify the partitioning through the options database, use one of the following

   -mat_partitioning_type parmetis, -mat_partitioning current

To see the partitioning result

   -mat_partitioning_view

The user can define additional partitionings; see MatPartitioningRegister(). 

3. .../petsc-3.13.0/src/mat/tutorials/ex17.c 给出一个例子关于空进程和大进程 

4. manual中 3.5 Partitioning 阐述了矩阵的分配机制，但是没有太明白IS在其中的作用

5. MatCreateAIJ=(MPI Comm comm,PetscInt m,PetscInt n,PetscInt M,PetscInt N,PetscInt
                 d_nz,PetscInt *d_nnz, PetscInt o_nz,PetscInt *o_nnz,Mat *A);
A is the newly created matrix, while the arguments m, M, and N, indicate the number of local rows and the number of global rows and columns, respectively. 
In the PETSc partitioning scheme, all the matrix columns are local and n is the number of columns corresponding to local part of a parallel vector. 
Either the local or global parameters can be replaced with PETSC_DECIDE, so that PETSc will determine them.
The matrix is stored with a fixed number of rows on each process, given by m, or determined by PETSc if m is PETSC_DECIDE.
If PETSC_DECIDE is not used for the arguments m and n, then the user must ensure that they are chosen to be compatible with the vectors. 
To do this, one first considers the matrix-vector product y = Ax. 
The m that is used in the matrix creation routine MatCreateAIJ() must match the local size used in the vector creation routine VecCreateMPI() for y. Likewise, the n used must match that used as the local size in VecCreateMPI() for x.

6. 做一个测试函数，利用gcge中ops生成矩阵A的分层矩阵，然后构造new_P_H (行多列少) 
   new_P_H的行分配与old_P_H同，但列分配自定义. 设定bigranks
   然后利用new_P_H调用PtAP，重新生成new_A_H, 已new_A_H生成对应的粗空间中的向量
   完成在pase/test/test_mg_gcge.c

2020.5.16 liyu
1. 有一个编译的BUG, 对于petsc app, MatSetSizes
In file included from /home/liyu/2020-WS/petsc/include/petscsys.h:1474:0,
                 from /home/liyu/2020-WS/petsc/include/petscoptions.h:6,
                 from ../../include/gcge_app_slepc.h:29,
                 from test_mg_gcge.c:19:
/home/liyu/2020-WS/petsc/include/petsclog.h:64:1: warning: parameter names (without types) in function declaration
 PETSC_EXTERN PetscErrorCode PetscInfoSetFromOptions(PetscOptions);
 ^~~~~~~~~~~~

但加入pase_convert.h之后再编译就没有这样的问题

2. 利用PETSc进行矩阵分层时，最粗层的矩阵会只在一个进程中存储!
   这个可能是由于，petsc在计算最粗层线性方程组时利用直接法
   但是其它层的矩阵是被所有进程平均分的

3. PCGAMGSetCoarseEqLim
Set maximum number of equations on coarsest grid. 

PCGAMGSetProcEqLim
Set number of equations to aim for per process on the coarse grids via processor reduction.



2020.5.19 liyu
1. 加入 gcge_linsol.c gcge_linsol.h
   将 CG BCG BMG 加入ops, 测试函数在pase/test/test_amg_gcge.c
2. 利用static去管理 linear_solver_workspace
3. 进行 PCG BPCG BAMG (重起了名字) 解方程之前，务必先SetUp

2020.5.20 liyu
1. RedistributeDataOfMultiGridMatrixOnEachProcess in pase/test/test_amg_gcge
2. 测试成功slepc下, MG矩阵生成，矩阵在各个进程中重分配，进行BAMG
   在测试中，工作空间被独立出来，不会和其它任何结构冲突
3. 虽然可以直接利用ops->MultiLinearSolver调用求解，但并没那么做, 之后再探讨
   注意：ops的默认LinearSolver和MultiLinearSolver已经更改分别使用内置的PCG和BPCG
         调用之前一定进行相应的Setup，这其实是在分配工作空间和设置相关参数
         预条件部分还没有实现

2020.5.31 liyu
1. Compute_W计算线性方程组时会判断LinearSolver和MultiLinearSolver
   所以不能在gcge_ops中设定默认的线性求解器
2. DenseMatDotDenseMat中如果nrows > 0 ncols > 0 中间维数==0
   那么生成一个nrows*ncols的零矩阵, 这种情况会发生在多向量在某个进程的行数为0的情况
   且还要做多向量的内积
3. pase中不能完全适应A_H只分配在部分进程的情况，虽然现在的版本可以正常运行
   但是，pase中对于这部分的优化基本没有，尤其是Iallreduce这部分，还是需要全局通信
   pase多向量内积时，依然需要进行全局的通信
4. 现在做到了什么：
   直接利用petsc的amg得到矩阵分层且每层矩阵的数据分在哪些进程可控制
   而且每一层矩阵对应的子通讯器也被存储了，可以进行针对性的MPI操作
   现在的pase比之前的应该会有小范围的提升，在V_H空间上的操作
   现在没做什么：
   现在的pase不能兼容对A_H只包含部分进程的情况的只进行pase_ops的优化
   在pase计算的内部都默认每个进程都包含aux部分，那么在延拓时也会用到这部分


2020.6.1 liyu
1. pase中的进程基于A_H可以划为两个组 PASE_MG_COMM[level][0] PASE_MG_COMM[level][1]
   通过PASE_MG_COMM_COLOR[level]标志当前进程属于哪个通讯域
2. 通过PASE_MG_AUX_COARSE_LEVEL_COMM 以及 PASE_MG_AUX_COARSE_LEVEL_COMM_COLOR 
   这两个指针记录当前辅助层的进程归属情况，也就是说，依然可以调用不同粗层做为辅助层
3. 组间通信，由于PASE_MG_COMM[level][1] 不包含A_H的行，但aux_h部分还是存在的，但值未知
   所以，在pase特征值计算完成后需要进行辅助多向量aux_h部分的广播
   即PASE_MG_COMM[level][0]的0进程将aux_h部分广播PASE_MG_COMM[level][1]的所有进程
   然后再进行prlong操作
4. 组间通信，pase_ops中的VecInnerProd和MultiVecInnerProd, 
   由于PASE_MG_COMM[level][1]中的aux_h部分的值完全未知，全局内积是不能靠进程本身的aux_h计算得到的
   所以，依然需要进程广播
   即PASE_MG_COMM[level][0]的0进程将value_ip部分广播PASE_MG_COMM[level][1]的所有进程
5. pase_ops中的MatDotVec MatDotMultiVec MultiVecInnerProd
   进行Iallreduce的时候只在PASE_MG_COMM[level][0]上进行，节省了一定的时间
   MatDotVec MatDotMultiVec 可以理解为，
   只要petsc自身矩阵向量操作对于那些拥有行数为零的进程有特殊操作的话, 这两个操作
   是完全在PASE_MG_COMM[level][0]上进行了。
   但MultiVecInMatDotVec还是需要广播，因为内积生成的是数，而矩阵乘以向量生成的向量
   数是全局的，向量是局部的

2020.6.6 liyu
1. 已经将正交化加入到ops里，但只写了G-S标准情况
2. 准备将特征值也包含在GCGE的ops中，重点是使得它的workspace完全可见
3. 重新整理app，并且使得eigsol对应的ops也能被接口赋值
   换句话说，这样只要create ops就可以做任何事情了
4. 对于正交化的改造，首先写一个WSubV这样的子函数，就是GCGE_SubOrthonormalization
   然后就是自身正交化VOrthSelf
   不同的方法可以有不同的VOrthSelf_二分法, VOrthSelf_BlockGS
   把这些子函数组合起来形成
   GCGE_Orth_二分法，GCGE_Orth_BlockGS
5. 对于EigenSolver的改造需要对不同运行参数下选择不同的正交化方法
6. 加入对UMFPACK的支持以及superLU
7. 加入对ani2D/3D的支持

2020.6.8 liyu
1. GCGE有一个BUG，不管blocksize取多少，都是10个10个算特征值(后面的特征向量不会被计算)，
   这导致了在做下属第二点时，
   进行正交化的失败，所以现在将pase中调用GCGE的迭代次数设为20, 这样每个特征值都可以被求到
2. 当每次光滑后，将v_h = v_h - I_H^h inv(B_H) (I_h^H B_h v_h)
   那么生成的A_Hh矩阵的hh部分接近一个对角矩阵，B_Hh矩阵的hh部分就是单位阵，且Hh部分是零
   需要注意的是 inv(B_H) I_h^H B_h v_h 的求解需要很精确，才能使得 aux_Hh 接近于零,
   它的精度与迭代求解inv(B_H)的精度相当, 如果有很好的直接法(并行)是最佳的.
   而且可以多次调用程序循环计算v_h = v_h - I_H^h inv(B_H) I_h^H B_h v_h，当前默认为2次
3. 另外，对新的v_h进行正交归一化的时候，精度只能保持在1e-16
4. 受制于上述两点，那么如果直接将B_Hh中的aux_Hh赋值为零，aux_hh赋值为单位矩阵，
   那么特征值的精度会相应收到影响
5. 也许是petsc生成的多层矩阵与进程数相关，当进程数提高的时候，整体的迭代次数变多
   多进程时，做了上述第二点后，迭代次数也会变化
   一个进程的时候，万事没问题。
